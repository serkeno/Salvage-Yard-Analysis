{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T07:02:57.487018Z",
     "start_time": "2024-07-23T07:02:57.404365Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import DataframeBuilder"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:02:57.503044Z",
     "start_time": "2024-07-23T07:02:57.491030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_type = \"TotalPartsSold\"\n",
    "presence_type = \"binary\""
   ],
   "id": "ce54772f832a85e7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:03:37.503883Z",
     "start_time": "2024-07-23T07:02:57.504044Z"
    }
   },
   "cell_type": "code",
   "source": "vehicle_presence_df = DataframeBuilder.vehicle_presence(presence_type=presence_type, vehicle_type=\"year_model\", target_type=target_type)",
   "id": "8285af8f27f3271b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:03:37.519608Z",
     "start_time": "2024-07-23T07:03:37.504900Z"
    }
   },
   "cell_type": "code",
   "source": "vehicle_presence_df.columns = vehicle_presence_df.columns.astype(str)",
   "id": "964fd3a5f8b60b52",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:03:37.676282Z",
     "start_time": "2024-07-23T07:03:37.521606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the machine learning model steps here, including training and testing LR\n",
    "# Separating features and target\n",
    "X = vehicle_presence_df.drop(columns=[target_type, 'Date'])\n",
    "y = vehicle_presence_df[target_type]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "id": "cb5405f3d989a526",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T08:34:57.795084Z",
     "start_time": "2024-07-23T07:03:37.677489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for tuning\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_percentage_error', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results_df = pd.DataFrame({\n",
    "    'n_estimators': results['param_n_estimators'],\n",
    "    'learning_rate': results['param_learning_rate'],\n",
    "    'max_depth': results['param_max_depth'],\n",
    "    'subsample': results['param_subsample'],\n",
    "    'mean_test_score': results['mean_test_score'],\n",
    "    'std_test_score': results['std_test_score']\n",
    "})\n",
    "\n",
    "print(results_df)"
   ],
   "id": "b1e83c6abf3f0339",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_estimators  learning_rate  max_depth  subsample  mean_test_score  \\\n",
      "0            100           0.01          3        0.8        -0.416921   \n",
      "1            100           0.01          3        0.9        -0.415993   \n",
      "2            100           0.01          3        1.0        -0.417231   \n",
      "3            200           0.01          3        0.8        -0.347207   \n",
      "4            200           0.01          3        0.9        -0.346124   \n",
      "..           ...            ...        ...        ...              ...   \n",
      "76           200           0.10          5        0.9        -0.351141   \n",
      "77           200           0.10          5        1.0        -0.350328   \n",
      "78           300           0.10          5        0.8        -0.362581   \n",
      "79           300           0.10          5        0.9        -0.359644   \n",
      "80           300           0.10          5        1.0        -0.355515   \n",
      "\n",
      "    std_test_score  \n",
      "0         0.053560  \n",
      "1         0.053991  \n",
      "2         0.054236  \n",
      "3         0.061408  \n",
      "4         0.062190  \n",
      "..             ...  \n",
      "76        0.072150  \n",
      "77        0.073854  \n",
      "78        0.071853  \n",
      "79        0.073723  \n",
      "80        0.074041  \n",
      "\n",
      "[81 rows x 6 columns]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T10:32:03.112948Z",
     "start_time": "2024-07-23T08:34:57.796067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_type = \"TotalPartsSold\"\n",
    "presence_type = \"continuous\"\n",
    "vehicle_presence_df = DataframeBuilder.vehicle_presence(presence_type=presence_type, vehicle_type=\"year_model\",\n",
    "                                                        target_type=target_type)\n",
    "vehicle_presence_df.columns = vehicle_presence_df.columns.astype(str)\n",
    "# Create the machine learning model steps here, including training and testing LR\n",
    "# Separating features and target\n",
    "X = vehicle_presence_df.drop(columns=[target_type, 'Date'])\n",
    "y = vehicle_presence_df[target_type]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for tuning\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_percentage_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results_df = pd.DataFrame({\n",
    "    'n_estimators': results['param_n_estimators'],\n",
    "    'learning_rate': results['param_learning_rate'],\n",
    "    'max_depth': results['param_max_depth'],\n",
    "    'subsample': results['param_subsample'],\n",
    "    'mean_test_score': results['mean_test_score'],\n",
    "    'std_test_score': results['std_test_score']\n",
    "})\n",
    "\n",
    "print(results_df)"
   ],
   "id": "29a347aa4acb2a82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_estimators  learning_rate  max_depth  subsample  mean_test_score  \\\n",
      "0            100           0.01          3        0.8        -0.415424   \n",
      "1            100           0.01          3        0.9        -0.414494   \n",
      "2            100           0.01          3        1.0        -0.414322   \n",
      "3            200           0.01          3        0.8        -0.345136   \n",
      "4            200           0.01          3        0.9        -0.343583   \n",
      "..           ...            ...        ...        ...              ...   \n",
      "76           200           0.10          5        0.9        -0.339255   \n",
      "77           200           0.10          5        1.0        -0.333698   \n",
      "78           300           0.10          5        0.8        -0.340052   \n",
      "79           300           0.10          5        0.9        -0.341725   \n",
      "80           300           0.10          5        1.0        -0.338827   \n",
      "\n",
      "    std_test_score  \n",
      "0         0.051048  \n",
      "1         0.050757  \n",
      "2         0.050787  \n",
      "3         0.056964  \n",
      "4         0.056682  \n",
      "..             ...  \n",
      "76        0.071982  \n",
      "77        0.066208  \n",
      "78        0.071202  \n",
      "79        0.073449  \n",
      "80        0.067861  \n",
      "\n",
      "[81 rows x 6 columns]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T12:04:34.141971Z",
     "start_time": "2024-07-23T10:32:03.114977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_type = \"TotalPrice\"\n",
    "presence_type = \"binary\"\n",
    "vehicle_presence_df = DataframeBuilder.vehicle_presence(presence_type=presence_type, vehicle_type=\"year_model\",\n",
    "                                                        target_type=target_type)\n",
    "vehicle_presence_df.columns = vehicle_presence_df.columns.astype(str)\n",
    "# Create the machine learning model steps here, including training and testing LR\n",
    "# Separating features and target\n",
    "X = vehicle_presence_df.drop(columns=[target_type, 'Date'])\n",
    "y = vehicle_presence_df[target_type]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for tuning\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_percentage_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results_df = pd.DataFrame({\n",
    "    'n_estimators': results['param_n_estimators'],\n",
    "    'learning_rate': results['param_learning_rate'],\n",
    "    'max_depth': results['param_max_depth'],\n",
    "    'subsample': results['param_subsample'],\n",
    "    'mean_test_score': results['mean_test_score'],\n",
    "    'std_test_score': results['std_test_score']\n",
    "})\n",
    "\n",
    "print(results_df)"
   ],
   "id": "4855e4567a157ea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_estimators  learning_rate  max_depth  subsample  mean_test_score  \\\n",
      "0            100           0.01          3        0.8        -0.513319   \n",
      "1            100           0.01          3        0.9        -0.511524   \n",
      "2            100           0.01          3        1.0        -0.511877   \n",
      "3            200           0.01          3        0.8        -0.452392   \n",
      "4            200           0.01          3        0.9        -0.450463   \n",
      "..           ...            ...        ...        ...              ...   \n",
      "76           200           0.10          5        0.9        -0.469887   \n",
      "77           200           0.10          5        1.0        -0.472357   \n",
      "78           300           0.10          5        0.8        -0.478235   \n",
      "79           300           0.10          5        0.9        -0.477695   \n",
      "80           300           0.10          5        1.0        -0.481579   \n",
      "\n",
      "    std_test_score  \n",
      "0         0.129354  \n",
      "1         0.128361  \n",
      "2         0.127840  \n",
      "3         0.145479  \n",
      "4         0.143069  \n",
      "..             ...  \n",
      "76        0.173252  \n",
      "77        0.188207  \n",
      "78        0.186866  \n",
      "79        0.180674  \n",
      "80        0.193964  \n",
      "\n",
      "[81 rows x 6 columns]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T14:01:07.273784Z",
     "start_time": "2024-07-23T12:04:34.142866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_type = \"TotalPrice\"\n",
    "presence_type = \"continuous\"\n",
    "vehicle_presence_df = DataframeBuilder.vehicle_presence(presence_type=presence_type, vehicle_type=\"year_model\",\n",
    "                                                        target_type=target_type)\n",
    "vehicle_presence_df.columns = vehicle_presence_df.columns.astype(str)\n",
    "# Create the machine learning model steps here, including training and testing LR\n",
    "# Separating features and target\n",
    "X = vehicle_presence_df.drop(columns=[target_type, 'Date'])\n",
    "y = vehicle_presence_df[target_type]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for tuning\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_percentage_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results_df = pd.DataFrame({\n",
    "    'n_estimators': results['param_n_estimators'],\n",
    "    'learning_rate': results['param_learning_rate'],\n",
    "    'max_depth': results['param_max_depth'],\n",
    "    'subsample': results['param_subsample'],\n",
    "    'mean_test_score': results['mean_test_score'],\n",
    "    'std_test_score': results['std_test_score']\n",
    "})\n",
    "\n",
    "print(results_df)"
   ],
   "id": "a5284b8161c622f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_estimators  learning_rate  max_depth  subsample  mean_test_score  \\\n",
      "0            100           0.01          3        0.8        -0.509976   \n",
      "1            100           0.01          3        0.9        -0.508042   \n",
      "2            100           0.01          3        1.0        -0.508248   \n",
      "3            200           0.01          3        0.8        -0.445118   \n",
      "4            200           0.01          3        0.9        -0.444508   \n",
      "..           ...            ...        ...        ...              ...   \n",
      "76           200           0.10          5        0.9        -0.426793   \n",
      "77           200           0.10          5        1.0        -0.419745   \n",
      "78           300           0.10          5        0.8        -0.432215   \n",
      "79           300           0.10          5        0.9        -0.430110   \n",
      "80           300           0.10          5        1.0        -0.425384   \n",
      "\n",
      "    std_test_score  \n",
      "0         0.124313  \n",
      "1         0.121538  \n",
      "2         0.120981  \n",
      "3         0.126739  \n",
      "4         0.127004  \n",
      "..             ...  \n",
      "76        0.123188  \n",
      "77        0.120905  \n",
      "78        0.126209  \n",
      "79        0.121021  \n",
      "80        0.116426  \n",
      "\n",
      "[81 rows x 6 columns]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Get the coefficients from the GBR model\n",
    "coef = gbr.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': coef})\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Display the top features\n",
    "print(feature_importance.head(10))"
   ],
   "id": "e446b032d77d6b0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
