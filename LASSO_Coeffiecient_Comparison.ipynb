{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-05T06:15:42.269076Z",
     "start_time": "2024-08-05T06:15:42.255024Z"
    }
   },
   "source": "# This is the final test of the LASSO model using the tuning parameters discovered during the tuning and training process. It will be used to compare coefficients of the model to a list of high-yield vehicles using domain knowledge provided by Luke.",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T06:15:42.284177Z",
     "start_time": "2024-08-05T06:15:42.271078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import DataframeBuilder"
   ],
   "id": "acdb6690db362d6b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T06:21:48.763588Z",
     "start_time": "2024-08-05T06:15:42.287188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_type_list = [\"TotalPrice\", \"TotalPartsSold\"]\n",
    "presence_type = \"continuous\"\n",
    "quantile_threshold_list = [0.1] # Controls the proportion of the lowest popularity vehicle types to be dropped\n",
    "imputation_multiplier_list = [4] # Controls the number of times the dataframe records are duplicated\n",
    "for target_type in target_type_list:\n",
    "    for quantile_threshold in quantile_threshold_list:\n",
    "        for imputation_multiplier in imputation_multiplier_list:\n",
    "            \n",
    "            vehicle_presence_df = DataframeBuilder.vehicle_presence(presence_type=presence_type, vehicle_type=\"year_model\", target_type=target_type)\n",
    "            \n",
    "            # Create additional records based on the imputation multiplier\n",
    "            vehicle_presence_df = pd.concat([vehicle_presence_df] * imputation_multiplier, ignore_index=True)\n",
    "            \n",
    "            if presence_type == \"continuous\":\n",
    "                \n",
    "              \n",
    "                if 'TotalPrice' in vehicle_presence_df.columns:\n",
    "                    sums = vehicle_presence_df.drop(columns=['TotalPrice', 'Date']).sum()\n",
    "                    \n",
    "                elif 'TotalPartsSold' in vehicle_presence_df.columns:\n",
    "                    sums = vehicle_presence_df.drop(columns=['TotalPartsSold', 'Date']).sum()\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Determine the threshold for the bottom x% of sums\n",
    "                    threshold = sums.quantile(quantile_threshold)\n",
    "                \n",
    "            \n",
    "                    # Find columns to drop\n",
    "                    cols_to_drop = sums[sums <= threshold].index.tolist()\n",
    "            \n",
    "                    # Drop columns from the dataframe\n",
    "                    vehicle_presence_df = vehicle_presence_df.drop(columns=cols_to_drop)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    print(\"Sums not found, TotalPrice and TotalPartsSold are not available in vehicle_presence_df.\")\n",
    "                    \n",
    "            vehicle_presence_df.columns = vehicle_presence_df.columns.astype(str)\n",
    "            \n",
    "            # Create the machine learning model steps here, including training and testing LR\n",
    "            # Separating features and target\n",
    "            X = vehicle_presence_df.drop(columns=[target_type, 'Date'])\n",
    "            y = vehicle_presence_df[target_type]\n",
    "            \n",
    "            # Normalize the features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Split the data for training and testing\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Initialize and fit the Lasso regression model\n",
    "            lasso = Lasso(alpha=0.001, random_state=42)\n",
    "            lasso.fit(X_train, y_train)\n",
    "            y_pred = lasso.predict(X_test)\n",
    "            \n",
    "            # TODO make sure this works as intended with coefficients paired with their feature name properly\n",
    "            coefficient_df = pd.DataFrame({\n",
    "            'Vehicle Year-Model': X.columns,\n",
    "            'Coefficient': lasso.coef_\n",
    "            })\n",
    "            coefficient_df['Ranking'] = coefficient_df['Coefficient'].rank(ascending=False, method='min')\n",
    "            coefficient_df = coefficient_df.sort_values(by='Ranking', ascending=False)\n",
    "            coefficient_df.to_csv(f\"LASSO_{target_type}_Final.csv\", index=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "         \n",
    "            \n",
    "           \n",
    "            \n",
    "        "
   ],
   "id": "b50723404c30e6dd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aljo9\\PycharmProjects\\NV_Picapart_Analysis\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.128e+05, tolerance: 2.471e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\aljo9\\PycharmProjects\\NV_Picapart_Analysis\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+04, tolerance: 5.129e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
