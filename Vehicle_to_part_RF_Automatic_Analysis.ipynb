{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T07:31:51.858305Z",
     "start_time": "2024-07-25T07:31:50.225988Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import DataframeBuilder"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T07:36:33.228089Z",
     "start_time": "2024-07-25T07:31:51.859317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_type_list = [\"TotalPartsSold\"]\n",
    "presence_type = \"continuous\"\n",
    "quantile_threshold_list = [0.1]  # Controls the proportion of the lowest popularity vehicle types to be dropped\n",
    "imputation_multiplier_list = [2, 3, 4]  # Controls the number of times the dataframe records are duplicated\n",
    "for target_type in target_type_list:\n",
    "    for quantile_threshold in quantile_threshold_list:\n",
    "        for imputation_multiplier in imputation_multiplier_list:\n",
    "            \n",
    "            vehicle_presence_df = DataframeBuilder.vehicle_presence(presence_type=presence_type, vehicle_type=\"year_model\", target_type=target_type)\n",
    "            \n",
    "            # Create additional records based on the imputation multiplier\n",
    "            vehicle_presence_df = pd.concat([vehicle_presence_df] * imputation_multiplier, ignore_index=True)\n",
    "            \n",
    "            if presence_type == \"continuous\":\n",
    "                \n",
    "              \n",
    "                if 'TotalPrice' in vehicle_presence_df.columns:\n",
    "                    sums = vehicle_presence_df.drop(columns=['TotalPrice', 'Date']).sum()\n",
    "                    \n",
    "                elif 'TotalPartsSold' in vehicle_presence_df.columns:\n",
    "                    sums = vehicle_presence_df.drop(columns=['TotalPartsSold', 'Date']).sum()\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Determine the threshold for the bottom x% of sums\n",
    "                    threshold = sums.quantile(quantile_threshold)\n",
    "                \n",
    "            \n",
    "                    # Find columns to drop\n",
    "                    cols_to_drop = sums[sums <= threshold].index.tolist()\n",
    "            \n",
    "                    # Drop columns from the dataframe\n",
    "                    vehicle_presence_df = vehicle_presence_df.drop(columns=cols_to_drop)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    print(\"Sums not found, TotalPrice and TotalPartsSold are not available in vehicle_presence_df.\")\n",
    "                    \n",
    "            vehicle_presence_df.columns = vehicle_presence_df.columns.astype(str)\n",
    "            \n",
    "            # Create the machine learning model steps here, including training and testing LR\n",
    "            # Separating features and target\n",
    "            X = vehicle_presence_df.drop(columns=[target_type, 'Date'])\n",
    "            y = vehicle_presence_df[target_type]\n",
    "            \n",
    "            # Normalize the features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Split the data for training and testing\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Initialize and fit the Lasso regression model\n",
    "            random_forest = RandomForestRegressor()\n",
    "            # Define the parameter grid\n",
    "            param_grid = {\n",
    "                  'n_estimators': [100, 200, 300],\n",
    "                  'max_depth': [10, 20],\n",
    "                  'max_features': ['log2'],\n",
    "                  'min_samples_split': [10],\n",
    "                  'min_samples_leaf': [4]\n",
    "            }\n",
    "            \n",
    "            # Use GridSearchCV for tuning\n",
    "            grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_percentage_error', return_train_score=True)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            # Get the results\n",
    "            results = grid_search.cv_results_\n",
    "            \n",
    "            # Create a DataFrame to display results\n",
    "            results_df = pd.DataFrame({\n",
    "                'n_estimators': results['param_n_estimators'],\n",
    "                'max_depth': results['param_max_depth'],\n",
    "                'max_features': results['param_max_features'],\n",
    "                'min_samples_split': results['param_min_samples_split'],\n",
    "                'min_samples_leaf': results['param_min_samples_leaf'],\n",
    "                'mean_test_score': results['mean_test_score'],\n",
    "                'std_test_score': results['std_test_score']\n",
    "            })\n",
    "            \n",
    "            results_df.to_csv(f'{target_type}_{quantile_threshold}_{imputation_multiplier}_RF_Results.csv', index=False)"
   ],
   "id": "fbd41d0f2511fe3d",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T07:44:53.324004Z",
     "start_time": "2024-07-25T07:36:33.230557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_type_list = [\"TotalPrice\"]\n",
    "presence_type = \"continuous\"\n",
    "quantile_threshold_list = [0.1]  # Controls the proportion of the lowest popularity vehicle types to be dropped\n",
    "imputation_multiplier_list = [2, 3, 4]  # Controls the number of times the dataframe records are duplicated\n",
    "for target_type in target_type_list:\n",
    "    for quantile_threshold in quantile_threshold_list:\n",
    "        for imputation_multiplier in imputation_multiplier_list:\n",
    "            \n",
    "            vehicle_presence_df = DataframeBuilder.vehicle_presence(presence_type=presence_type, vehicle_type=\"year_model\", target_type=target_type)\n",
    "            \n",
    "            # Create additional records based on the imputation multiplier\n",
    "            vehicle_presence_df = pd.concat([vehicle_presence_df] * imputation_multiplier, ignore_index=True)\n",
    "            \n",
    "            if presence_type == \"continuous\":\n",
    "                \n",
    "              \n",
    "                if 'TotalPrice' in vehicle_presence_df.columns:\n",
    "                    sums = vehicle_presence_df.drop(columns=['TotalPrice', 'Date']).sum()\n",
    "                    \n",
    "                elif 'TotalPartsSold' in vehicle_presence_df.columns:\n",
    "                    sums = vehicle_presence_df.drop(columns=['TotalPartsSold', 'Date']).sum()\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Determine the threshold for the bottom x% of sums\n",
    "                    threshold = sums.quantile(quantile_threshold)\n",
    "                \n",
    "            \n",
    "                    # Find columns to drop\n",
    "                    cols_to_drop = sums[sums <= threshold].index.tolist()\n",
    "            \n",
    "                    # Drop columns from the dataframe\n",
    "                    vehicle_presence_df = vehicle_presence_df.drop(columns=cols_to_drop)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    print(\"Sums not found, TotalPrice and TotalPartsSold are not available in vehicle_presence_df.\")\n",
    "                    \n",
    "            vehicle_presence_df.columns = vehicle_presence_df.columns.astype(str)\n",
    "            \n",
    "            # Create the machine learning model steps here, including training and testing LR\n",
    "            # Separating features and target\n",
    "            X = vehicle_presence_df.drop(columns=[target_type, 'Date'])\n",
    "            y = vehicle_presence_df[target_type]\n",
    "            \n",
    "            # Normalize the features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Split the data for training and testing\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Initialize and fit the Lasso regression model\n",
    "            random_forest = RandomForestRegressor()\n",
    "            # Define the parameter grid\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, 30],\n",
    "                'max_features': ['log2'],\n",
    "                'min_samples_split': [5, 10],\n",
    "                'min_samples_leaf': [4]\n",
    "            }\n",
    "            \n",
    "            # Use GridSearchCV for tuning\n",
    "            grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_percentage_error', return_train_score=True)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            # Get the results\n",
    "            results = grid_search.cv_results_\n",
    "            \n",
    "            # Create a DataFrame to display results\n",
    "            results_df = pd.DataFrame({\n",
    "                'n_estimators': results['param_n_estimators'],\n",
    "                'max_depth': results['param_max_depth'],\n",
    "                'max_features': results['param_max_features'],\n",
    "                'min_samples_split': results['param_min_samples_split'],\n",
    "                'min_samples_leaf': results['param_min_samples_leaf'],\n",
    "                'mean_test_score': results['mean_test_score'],\n",
    "                'std_test_score': results['std_test_score']\n",
    "            })\n",
    "            \n",
    "            results_df.to_csv(f'{target_type}_{quantile_threshold}_{imputation_multiplier}_RF_Results.csv', index=False)"
   ],
   "id": "8285af8f27f3271b",
   "execution_count": 3,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
